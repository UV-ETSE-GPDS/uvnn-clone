{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "___\n",
    "\n",
    "The work so far has been to implement kind of \"modular mini framework\" which takes the data, trains different classifiers (autoencoders) and generates perforamnce, weights and topology. The main class is in clfpipeline.py which is composed of preprocessors, readers, and classifiers. as a classifier so far it includes softmax and MLP, and sparse and stacked autoencoders. We will see how the process goes below, but it might be useful to review overal structure.\n",
    "\n",
    "\n",
    "__Most of the Classes modules and functions are  well documented__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the repository structure\n",
    "___\n",
    "short overfiew of directories in __uvnn__ directory:\n",
    "\n",
    "1. clfpipeline.py : main class which contains classifiers, readers and preprocessors, descirbed above.\n",
    "2. __clssifiers__: package consisting of classifiers and autoencoders:\n",
    "    * __nn directory__: base classes for MLPs: It includes training with SGD(constant step size) and evaluating errors\n",
    "    * __autoencoder_sparse.py__: one hidden layer sparse autoencoder, which tries to regularize the activation layer.\n",
    "    * __mlp.py__: multilayer perceptron with any number of hidden layers and different activation functio\n",
    "    * __mlp_2layer and mlp_3layer__: implementation of 2 and 3 layer networks explicitely. $Wx +b1, Ux+b2$ without using cycles. I just leave it there because it might be easier for someone to debug and understand backpropagation and gradients.  \n",
    "    * __softmax.py__: softmax regression \n",
    "    * __misc.py__: some helper functions\n",
    "\n",
    "3. __output__: folder for keeping network weights and benchmarks, for each dataset there is a folder inside.\n",
    "4. __input__: folder from where the program reads the data, this is not included in the repository as the files are usually large\n",
    "5. __utils/__: folder for pipeline modules, inside there are:\n",
    "    * __readers.py__ CSV data reader for classifiers, if there are other readers necessary they will go here\n",
    "    * __Preprocessors.py__ different preprocessors for classifier pipeline, Main function of this class is to preprocess data and provide train, test, val splits. So far there is only one BasicePreprocessor, which does mean substriction and after that dividing by the SD. There is also autoencoderPP preprocessor for autoencoder\n",
    "6. __config/__: config files(includes almost everything, method,  network parameters datasets, preprocessors and readers). there is a simple file smaple.yaml which gives and idea, loading configs from file isn't done yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up and load modules nothing special here\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from utils.readers import CsvReader\n",
    "from clfpipeline import Clfpipeline\n",
    "from classifiers.mlp import MLP\n",
    "from classifiers.misc import merge_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP classifier example\n",
    "\n",
    "mlp uses softmax function to turn the final layer into probabilites of classes, The loss to optimize is the average cross entropy, activation function can be passed either tanh or sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a csv_reader class for pipeline object, we pass filename of inputs and targets\n",
    "\n",
    "csv_reader = CsvReader(has_header=False, \n",
    "                       fn='input/trunc_mnist/trunc_mnist20x20_inputs.csv', \n",
    "                       fn_labels='input/trunc_mnist/trunc_mnist20x20_targets.csv')\n",
    "\n",
    "# we set up a main object 'pipe' which preprocesses the data before training \n",
    "pipe = Clfpipeline(csv_reader, load_now=True)\n",
    "digits_input = pipe.X\n",
    "digits_targets = pipe.y\n",
    "n_samples = pipe.X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the parameter for classifiers and train, detailed description of each parameter will be kept on github readme.\n",
    "Note these parameters aren't optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.mlp import MLP\n",
    "conf_clf = {'dims':(400, 50, 20, 15, 10), \n",
    "            'alpha':0.001,                \n",
    "            'reg':0.01,\n",
    "            'activation':'tanh'}\n",
    "\n",
    "conf_train = {'batchsize':32,\n",
    "              'costevery': 50,\n",
    "               'nepoch': 500,\n",
    "              'acc_batch':False,\n",
    "              'opt':'rmsprop',\n",
    "              'tolerance':0.01,\n",
    "              'loss_metric':'accuracy'}\n",
    "fullconf = merge_dicts(conf_clf, conf_train)\n",
    "pipe.set_classifier(MLP(**conf_clf))\n",
    "pipe.train(**conf_train)\n",
    "pipe.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call pipe save_weight function if we wont to save weights and topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe.save_weights(algo_name=\"AE_EXAMPLE\", dataset_name=\"TRUNCMNIST\", confs=fullconf, folder=\"output/truncmnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder Example\n",
    "\n",
    "Sparse autoencoder has an average squared error as the loss fucntion, activation function is sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.autoencoder_sparse import AutoEncoderSparse\n",
    "from utils.preprocessors import AutoEncoderPP\n",
    "csv_reader = CsvReader(has_header=False, fn='input/trunc_mnist/trunc_mnist20x20_inputs.csv')\n",
    "pipe = Clfpipeline(csv_reader, PreProc=AutoEncoderPP, load_now=True)\n",
    "\n",
    "conf_clf = {'dims':(400, 100, 400), \n",
    "            'alpha':0.001,\n",
    "            #'alpha':0.000095, \n",
    "            'reg':0.0000,\n",
    "            'beta':0,\n",
    "            'ro':0.05}\n",
    "\n",
    "conf_train = {'batchsize':-1,\n",
    "              'costevery': 1,\n",
    "               'nepoch': 500,\n",
    "                'acc_batch':True,\n",
    "                'opt':'rmsprop',\n",
    "                'loss_metric':'MSE',\n",
    "                 'tolerance':0.01}\n",
    "fullconf = merge_dicts(conf_clf, conf_train)\n",
    "\n",
    "#aec = AutoEncoderSparse(**conf_clf)\n",
    "#aec.grad_check(pipe.X[0], pipe.X[0])\n",
    "pipe.set_classifier(AutoEncoderSparse(**conf_clf))\n",
    "pipe.train(**conf_train)\n",
    "pipe.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse AE on Full mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.preprocessors import MnistPP\n",
    "csv_reader = CsvReader(fn='input/kaggle_set/train.csv',has_header=True, label_pos=0)\n",
    "#csv_reader = CsvReader(has_header=False, fn='input/trunc_mnist/trunc_mnist20x20_inputs.csv')\n",
    "pipe = Clfpipeline(csv_reader, PreProc=MnistPP, load_now=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.autoencoder_sparse import AutoEncoderSparse\n",
    "from utils.preprocessors import AutoEncoderPP\n",
    "conf_clf = {'dims':(784, 196, 784), \n",
    "            'alpha':0.001,\n",
    "            #'alpha':0.000095, \n",
    "            'reg':0.0000,\n",
    "            'beta':3,\n",
    "            'ro':0.1}\n",
    "conf_train = {'batchsize':600,\n",
    "              'costevery': 100,\n",
    "               'nepoch': 500000,\n",
    "                'acc_batch':True,\n",
    "                'opt':'rmsprop',\n",
    "                'loss_metric':'MSE',\n",
    "                 'tolerance':-1}\n",
    "fullconf = merge_dicts(conf_clf, conf_train)\n",
    "#aec = AutoEncoderSparse(**conf_clf)\n",
    "#aec.grad_check(pipe.X[0], pipe.X[0])\n",
    "pipe.set_classifier(AutoEncoderSparse(**conf_clf))\n",
    "pipe.train(**conf_train)\n",
    "pipe.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can visualize how sparse autoencoder predicts digits\n",
    "First row is the input, second row is the output of the AE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.misc import vis_images_truncmnist\n",
    "vis_images_truncmnist(pipe.preprocessor.X, pipe.classifier, 28, 28, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_input = pipe.preprocessor.X\n",
    "print cur_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_hidden = pipe.classifier.predict_hidden(cur_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_mnst_output = pipe.y\n",
    "print ae_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = pipe.classifier.get_weights()[0][:,:-1]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        n = i * 16 + j\n",
    "        if n >= len(W):\n",
    "            break\n",
    "        ax = plt.subplot(16, 16, n + 1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.imshow(W[n].reshape(28, 28))\n",
    "#plt.imshow(W[7].reshape(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoder on truncated mnist\n",
    "Let's make and train autoencoder with 2 hidden layers and with softmax function as the final activation to predict mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.autoencoder_sparse import AutoEncoderSparse\n",
    "from utils.readers import DummyReader\n",
    "\n",
    "csv_reader = CsvReader(has_header=False, fn='input/trunc_mnist/trunc_mnist20x20_inputs.csv')\n",
    "pipe = Clfpipeline(csv_reader, PreProc=AutoEncoderPP, load_now=True)\n",
    "cur_input = pipe.X\n",
    "dims = (400, 100, 30, 10)\n",
    "\n",
    "\n",
    "Ws = []  # stacked layers\n",
    "for i in range(len(dims) - 2):\n",
    "    sparse_dims = (dims[i], dims[i + 1], dims[i])\n",
    "    print sparse_dims\n",
    "    conf_sparse_clf = {'dims': sparse_dims,\n",
    "                       'alpha':0.001, \n",
    "                       'reg':0.0000,\n",
    "                       'beta':0,\n",
    "                       'ro':0.1}\n",
    "    conf_sparse_train = {'batchsize':-1,\n",
    "                          'costevery': 1,\n",
    "                          'nepoch': 1000,\n",
    "                          'acc_batch':True,\n",
    "                          'opt':'rmsprop',\n",
    "                          'loss_metric':'MSE',\n",
    "                          'tolerance':0.1}\n",
    "    ae_sparse = AutoEncoderSparse(**conf_sparse_clf)\n",
    "    pipe.set_classifier(ae_sparse)\n",
    "    print 'Starting Training # %d sparse auto encoder' % (i + 1) \n",
    "    print 'dims are (%d %d %d)' % sparse_dims\n",
    "    pipe.train(**conf_sparse_train)\n",
    "    print 'Finised Training'\n",
    "    pipe.plot()\n",
    "    # extract W from first layer of sparse AE\n",
    "    W = ae_sparse.get_weights()[0][:,:-1]\n",
    "    Ws.append(W)\n",
    "    cur_input = ae_sparse.predict_hidden(cur_input)\n",
    "    print 'cur_input shape is', cur_input.shape\n",
    "    \n",
    "    # now build up a new classifier for the next autoencoder, for which the \n",
    "    # input will be the hidden activations of current one\n",
    "    pipe = Clfpipeline(DummyReader(X=cur_input, y=cur_input), PreProc=AutoEncoderPP)\n",
    "\n",
    "    \n",
    "# train last stacked layer with softmax\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training softmax regression with the input which are features from last autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_input_cent = cur_input - np.mean(cur_input, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr_conf = {'dims':(30, 10),\n",
    "          'alpha':0.001,\n",
    "          'reg':0,\n",
    "          'activation':'sigmoid'\n",
    "          }\n",
    "sr_train_conf = {'batchsize':700,\n",
    "                'costevery':200,\n",
    "                'nepoch':100000,\n",
    "                'acc_batch':False,\n",
    "                'opt':'rmsprop',\n",
    "                'loss_metric':'accuracy',\n",
    "                'tolerance':-10}\n",
    "\n",
    "sr_pipe = Clfpipeline(DummyReader(X=cur_input_cent, y=kaggle_mnst_output), load_now=True)\n",
    "sr_pipe.set_classifier(MLP(**sr_conf))\n",
    "sr_pipe.train(**sr_train_conf)\n",
    "\n",
    "# Extract the last layer\n",
    "last_W = sr_pipe.get_weights()[0][:,:-1]\n",
    "Ws.append(last_W)\n",
    "print 'we have %d pretrained weights now' %(len (Ws))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  fine tune the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### now build the mlp and initialize weights with sparse aes\n",
    "csv_reader = CsvReader(has_header=False, \n",
    "                       fn='input/trunc_mnist/trunc_mnist20x20_inputs.csv', \n",
    "                       fn_labels='input/trunc_mnist/trunc_mnist20x20_targets.csv')\n",
    "\n",
    "# we set up a main object 'pipe' which preprocesses the data before training \n",
    "pipe = Clfpipeline(csv_reader, load_now=True)\n",
    "conf_clf = {'dims':dims, \n",
    "            'alpha':0.001,                \n",
    "            'reg':0.01,\n",
    "            'activation':'sigmoid',\n",
    "            'init_weights':Ws\n",
    "           }\n",
    "\n",
    "conf_train = {'batchsize':32,\n",
    "              'costevery': 50,\n",
    "               'nepoch': 500,\n",
    "              'acc_batch':False,\n",
    "              'opt':'rmsprop',\n",
    "              'tolerance':-10,\n",
    "              'loss_metric':'accuracy'}\n",
    "fullconf = merge_dicts(conf_clf, conf_train)\n",
    "pipe.set_classifier(MLP(**conf_clf))\n",
    "pipe.train(**conf_train)\n",
    "pipe.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}